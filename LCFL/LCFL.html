<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>LangChain 2.2 â€” Advanced LCEL Patterns</title>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=IBM+Plex+Sans:wght@300;400;500;600&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet"/>
  <style>
    :root {
      --bg:       #f5f0e8;
      --bg2:      #ede8df;
      --surface:  #faf7f2;
      --surface2: #f0ebe0;
      --border:   #d4c9b4;
      --border2:  #c4b89e;
      --ink:      #1a1208;
      --ink2:     #3d2f18;
      --muted:    #7a6a50;
      --muted2:   #9a8a6e;
      --red:      #c0392b;
      --orange:   #d35400;
      --teal:     #1a6b5a;
      --blue:     #1a3f6b;
      --purple:   #4a1f6b;
      --gold:     #8a6a00;
      --red-bg:   #f9eae8;
      --orange-bg:#faf0e6;
      --teal-bg:  #e6f4f0;
      --blue-bg:  #e6eef8;
      --purple-bg:#f0e8f8;
      --gold-bg:  #f8f4e0;
      --code-bg:  #1c150a;
      --code-border: #2e2410;
    }
    *{box-sizing:border-box;margin:0;padding:0;}
    body{
      background:var(--bg);
      color:var(--ink);
      font-family:'IBM Plex Sans',sans-serif;
      font-size:15px;
      line-height:1.75;
    }

    /* â”€â”€ GRAIN OVERLAY â”€â”€ */
    body::before{
      content:'';
      position:fixed;inset:0;
      background-image:url("data:image/svg+xml,%3Csvg viewBox='0 0 256 256' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='noise'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.9' numOctaves='4' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23noise)' opacity='0.04'/%3E%3C/svg%3E");
      pointer-events:none;z-index:0;opacity:0.5;
    }
    body > *{position:relative;z-index:1;}

    /* â”€â”€ NAV â”€â”€ */
    nav{
      background:var(--ink);
      padding:13px 40px;
      display:flex;gap:4px;flex-wrap:wrap;align-items:center;
      position:sticky;top:0;z-index:100;
    }
    .nav-label{
      color:#7a6a50;font-size:10px;
      font-family:'IBM Plex Mono',monospace;letter-spacing:2px;margin-right:8px;
    }
    nav a{
      color:#9a8a6e;text-decoration:none;font-size:11px;
      font-family:'IBM Plex Mono',monospace;
      padding:4px 11px;border-radius:3px;border:1px solid #2e2410;
      transition:all 0.15s;letter-spacing:0.3px;
    }
    nav a:hover{color:#f5f0e8;border-color:#5a4a30;background:#2e2410;}

    /* â”€â”€ HERO â”€â”€ */
    .hero{
      background:var(--ink);
      padding:72px 52px 60px;
      border-bottom:4px solid var(--orange);
      position:relative;overflow:hidden;
    }
    .hero-grid{
      position:absolute;inset:0;
      background-image:
        linear-gradient(rgba(255,255,255,0.03) 1px,transparent 1px),
        linear-gradient(90deg,rgba(255,255,255,0.03) 1px,transparent 1px);
      background-size:40px 40px;
    }
    .hero-eyebrow{
      display:inline-flex;align-items:center;gap:8px;
      border:1px solid #3d2f18;background:#2e2410;
      color:#9a8a6e;font-family:'IBM Plex Mono',monospace;
      font-size:10px;letter-spacing:3px;text-transform:uppercase;
      padding:5px 14px;border-radius:2px;margin-bottom:24px;
    }
    .hero h1{
      font-family:'Space Grotesk',sans-serif;
      font-size:clamp(2rem,5.5vw,3.6rem);
      font-weight:700;line-height:1.0;
      color:#f5f0e8;
      margin-bottom:20px;
      letter-spacing:-1px;
    }
    .hero h1 em{
      font-style:normal;
      color:var(--orange);
      border-bottom:3px solid var(--orange);
    }
    .hero-desc{
      color:#7a6a50;max-width:580px;
      font-size:15px;line-height:1.7;font-weight:300;
    }
    .hero-pills{display:flex;flex-wrap:wrap;gap:8px;margin-top:26px;}
    .hpill{
      font-size:11px;font-family:'IBM Plex Mono',monospace;
      padding:5px 13px;border:1px solid;border-radius:2px;
    }
    .hp-r{background:rgba(192,57,43,0.15);color:#e07060;border-color:rgba(192,57,43,0.3);}
    .hp-o{background:rgba(211,84,0,0.15);color:#e08040;border-color:rgba(211,84,0,0.3);}
    .hp-t{background:rgba(26,107,90,0.15);color:#40a090;border-color:rgba(26,107,90,0.3);}
    .hp-b{background:rgba(26,63,107,0.15);color:#4080c0;border-color:rgba(26,63,107,0.3);}
    .hp-p{background:rgba(74,31,107,0.15);color:#9060c0;border-color:rgba(74,31,107,0.3);}

    /* â”€â”€ LAYOUT â”€â”€ */
    .page{max-width:960px;margin:0 auto;padding:56px 36px;display:flex;flex-direction:column;gap:76px;}

    /* â”€â”€ SECTION HEADER â”€â”€ */
    .sh{margin-bottom:28px;border-left:4px solid var(--border2);padding-left:18px;}
    .sh-row{display:flex;align-items:center;gap:10px;margin-bottom:4px;}
    .snum{
      font-family:'IBM Plex Mono',monospace;font-size:10px;
      padding:3px 9px;border-radius:2px;letter-spacing:2px;font-weight:600;
      border:1px solid;
    }
    .sn-r{background:var(--red-bg);color:var(--red);border-color:rgba(192,57,43,0.25);}
    .sn-o{background:var(--orange-bg);color:var(--orange);border-color:rgba(211,84,0,0.25);}
    .sn-t{background:var(--teal-bg);color:var(--teal);border-color:rgba(26,107,90,0.25);}
    .sn-b{background:var(--blue-bg);color:var(--blue);border-color:rgba(26,63,107,0.25);}
    .sn-p{background:var(--purple-bg);color:var(--purple);border-color:rgba(74,31,107,0.25);}
    .sn-g{background:var(--gold-bg);color:var(--gold);border-color:rgba(138,106,0,0.25);}
    .stitle{font-family:'Space Grotesk',sans-serif;font-size:1.55rem;font-weight:700;color:var(--ink);letter-spacing:-0.5px;}
    .sdesc{color:var(--muted);font-size:13.5px;margin-top:4px;}

    /* â”€â”€ CALLOUTS â”€â”€ */
    .box{border-radius:3px;padding:16px 20px;margin:14px 0;font-size:13.5px;border:1px solid;border-left:4px solid;}
    .box strong{display:block;margin-bottom:3px;font-size:12px;font-family:'IBM Plex Mono',monospace;letter-spacing:1px;text-transform:uppercase;}
    .box p,.box{color:var(--muted);}
    .box-r{background:var(--red-bg);border-color:rgba(192,57,43,0.2);border-left-color:var(--red);}
    .box-r strong{color:var(--red);}
    .box-o{background:var(--orange-bg);border-color:rgba(211,84,0,0.2);border-left-color:var(--orange);}
    .box-o strong{color:var(--orange);}
    .box-t{background:var(--teal-bg);border-color:rgba(26,107,90,0.2);border-left-color:var(--teal);}
    .box-t strong{color:var(--teal);}
    .box-b{background:var(--blue-bg);border-color:rgba(26,63,107,0.2);border-left-color:var(--blue);}
    .box-b strong{color:var(--blue);}
    .box-p{background:var(--purple-bg);border-color:rgba(74,31,107,0.2);border-left-color:var(--purple);}
    .box-p strong{color:var(--purple);}
    .box-g{background:var(--gold-bg);border-color:rgba(138,106,0,0.2);border-left-color:var(--gold);}
    .box-g strong{color:var(--gold);}

    /* â”€â”€ CODE â”€â”€ */
    pre{
      background:var(--code-bg);
      border:1px solid var(--code-border);
      border-radius:4px;
      padding:24px 26px;
      overflow-x:auto;
      font-family:'IBM Plex Mono',monospace;
      font-size:12.5px;line-height:1.8;
      margin:16px 0;position:relative;
    }
    .code-label{
      position:absolute;top:0;left:0;right:0;
      background:#2e2410;border-bottom:1px solid var(--code-border);
      padding:5px 16px;
      font-size:10px;font-family:'IBM Plex Mono',monospace;
      color:#7a6a50;letter-spacing:1.5px;text-transform:uppercase;
    }
    pre.labeled{padding-top:42px;}
    .kw{color:#e07060;} .fn{color:#80b0e0;} .str{color:#90c070;}
    .cm{color:#4a3a24;font-style:italic;} .cls{color:#e0b060;}
    .op{color:#60b0c0;} .num{color:#c08060;} .dec{color:#e09080;}
    code{
      font-family:'IBM Plex Mono',monospace;
      background:var(--surface2);border:1px solid var(--border);
      padding:2px 7px;border-radius:3px;font-size:12px;color:var(--orange);
    }

    /* â”€â”€ BRANCH DIAGRAM â”€â”€ */
    .branch-diagram{
      background:var(--surface);border:1px solid var(--border);
      border-radius:4px;padding:28px;margin:16px 0;
      font-family:'IBM Plex Mono',monospace;
    }
    .bd-row{display:flex;align-items:flex-start;gap:0;}
    .bd-input{
      background:var(--surface2);border:1px solid var(--border);
      border-radius:3px;padding:10px 16px;font-size:12px;color:var(--ink2);
      align-self:center;white-space:nowrap;
    }
    .bd-arrow{color:var(--border2);font-size:22px;padding:0 8px;align-self:center;}
    .bd-router{
      background:var(--gold-bg);border:1px solid rgba(138,106,0,0.3);
      border-radius:3px;padding:10px 16px;font-size:12px;color:var(--gold);
      align-self:center;white-space:nowrap;font-weight:600;
    }
    .bd-branches{display:flex;flex-direction:column;gap:8px;margin-left:8px;flex:1;}
    .bd-branch{
      display:flex;align-items:center;gap:8px;
    }
    .bd-cond{
      font-size:11px;color:var(--muted);border-right:1px dashed var(--border2);
      padding-right:10px;min-width:120px;text-align:right;
    }
    .bd-chain{
      border-radius:3px;padding:8px 14px;font-size:11.5px;flex:1;
    }
    .bdc-r{background:var(--red-bg);color:var(--red);border:1px solid rgba(192,57,43,0.2);}
    .bdc-t{background:var(--teal-bg);color:var(--teal);border:1px solid rgba(26,107,90,0.2);}
    .bdc-b{background:var(--blue-bg);color:var(--blue);border:1px solid rgba(26,63,107,0.2);}
    .bdc-g{background:var(--gold-bg);color:var(--gold);border:1px solid rgba(138,106,0,0.2);}

    /* â”€â”€ FALLBACK DIAGRAM â”€â”€ */
    .fallback-diagram{
      background:var(--surface);border:1px solid var(--border);
      border-radius:4px;padding:24px;margin:16px 0;
    }
    .fd-row{
      display:flex;align-items:center;gap:10px;margin-bottom:10px;
      font-family:'IBM Plex Mono',monospace;font-size:12px;
    }
    .fd-row:last-child{margin-bottom:0;}
    .fd-num{
      width:24px;height:24px;border-radius:50%;
      display:flex;align-items:center;justify-content:center;
      font-size:11px;font-weight:600;flex-shrink:0;
    }
    .fn-ok{background:var(--teal-bg);color:var(--teal);border:1px solid rgba(26,107,90,0.3);}
    .fn-fail{background:var(--red-bg);color:var(--red);border:1px solid rgba(192,57,43,0.3);}
    .fd-box{
      padding:8px 14px;border-radius:3px;flex:1;
    }
    .fdb-primary{background:var(--blue-bg);color:var(--blue);border:1px solid rgba(26,63,107,0.2);}
    .fdb-fallback{background:var(--orange-bg);color:var(--orange);border:1px solid rgba(211,84,0,0.2);}
    .fdb-final{background:var(--teal-bg);color:var(--teal);border:1px solid rgba(26,107,90,0.2);}
    .fd-status{
      font-size:10px;padding:2px 8px;border-radius:2px;white-space:nowrap;
    }
    .fs-try{background:var(--surface2);color:var(--muted);border:1px solid var(--border);}
    .fs-fail{background:var(--red-bg);color:var(--red);border:1px solid rgba(192,57,43,0.2);}
    .fs-ok{background:var(--teal-bg);color:var(--teal);border:1px solid rgba(26,107,90,0.2);}
    .fd-connector{padding-left:17px;border-left:2px dashed var(--border2);margin-left:12px;color:var(--muted2);font-size:11px;font-family:'IBM Plex Mono',monospace;padding-top:4px;padding-bottom:4px;}

    /* â”€â”€ RETRY DIAGRAM â”€â”€ */
    .retry-timeline{
      background:var(--surface);border:1px solid var(--border);
      border-radius:4px;padding:22px 26px;margin:16px 0;
    }
    .rt-row{display:flex;align-items:center;gap:0;margin-bottom:12px;}
    .rt-row:last-child{margin-bottom:0;}
    .rt-attempt{
      font-family:'IBM Plex Mono',monospace;font-size:10px;
      color:var(--muted);min-width:70px;
    }
    .rt-bar{height:8px;border-radius:2px;position:relative;}
    .rb-fail{background:rgba(192,57,43,0.4);}
    .rb-ok{background:rgba(26,107,90,0.5);}
    .rt-status{
      font-family:'IBM Plex Mono',monospace;font-size:10px;
      margin-left:10px;padding:2px 8px;border-radius:2px;
    }
    .rs-fail{background:var(--red-bg);color:var(--red);}
    .rs-ok{background:var(--teal-bg);color:var(--teal);}
    .rt-wait{
      font-family:'IBM Plex Mono',monospace;font-size:10px;
      color:var(--muted2);margin-left:8px;font-style:italic;
    }

    /* â”€â”€ BIND PARAMS TABLE â”€â”€ */
    .params-grid{display:grid;grid-template-columns:1fr 1fr;gap:12px;margin:14px 0;}
    @media(max-width:600px){.params-grid{grid-template-columns:1fr;}}
    .param-card{
      background:var(--surface);border:1px solid var(--border);
      border-radius:3px;padding:18px;
    }
    .param-name{
      font-family:'IBM Plex Mono',monospace;font-size:12px;
      font-weight:600;color:var(--purple);margin-bottom:6px;display:block;
    }
    .param-card p{font-size:13px;color:var(--muted);}

    /* â”€â”€ METHOD COMPARISON TABLE â”€â”€ */
    .table-wrap{overflow-x:auto;margin:16px 0;}
    table{width:100%;border-collapse:collapse;font-size:13.5px;min-width:480px;}
    th{
      background:var(--ink);color:#9a8a6e;
      font-family:'IBM Plex Mono',monospace;font-size:10px;text-transform:uppercase;
      letter-spacing:2px;padding:12px 16px;text-align:left;
    }
    td{padding:12px 16px;border-bottom:1px solid var(--border);color:var(--muted);vertical-align:top;}
    tr:last-child td{border-bottom:none;}
    tr:hover td{background:var(--surface2);}
    td:first-child{font-family:'IBM Plex Mono',monospace;font-size:12px;color:var(--orange);}

    /* â”€â”€ SUMMARY â”€â”€ */
    .summary-grid{display:grid;grid-template-columns:1fr 1fr;gap:14px;}
    @media(max-width:600px){.summary-grid{grid-template-columns:1fr;}}
    .sum-card{
      background:var(--surface);border:1px solid var(--border);
      border-top:3px solid;border-radius:3px;padding:20px;
    }
    .sc-r{border-top-color:var(--red);}
    .sc-o{border-top-color:var(--orange);}
    .sc-t{border-top-color:var(--teal);}
    .sc-b{border-top-color:var(--blue);}
    .sc-p{border-top-color:var(--purple);}
    .sum-card h4{
      font-family:'Space Grotesk',sans-serif;font-size:13px;
      font-weight:700;color:var(--ink);margin-bottom:10px;
    }
    .sum-card li{
      font-size:12.5px;color:var(--muted);list-style:none;
      padding:3px 0;display:flex;gap:8px;align-items:flex-start;
    }
    .sum-card li::before{content:'â†’';flex-shrink:0;color:var(--muted2);}

    hr{border:none;border-top:1px solid var(--border);}
    footer{
      text-align:center;padding:32px;
      color:var(--muted2);font-size:11px;
      font-family:'IBM Plex Mono',monospace;
      letter-spacing:1px;
      background:var(--ink);color:#4a3a24;
      border-top:4px solid var(--orange);
    }
  </style>
</head>
<body>

<nav>
  <span class="nav-label">2.2</span>
  <a href="#branch">RunnableBranch</a>
  <a href="#routing">Dynamic Routing</a>
  <a href="#fallbacks">Fallbacks</a>
  <a href="#retry">Retry</a>
  <a href="#bind">Bind</a>
  <a href="#summary">Summary</a>
</nav>

<div class="hero">
  <div class="hero-grid"></div>
  <div class="hero-eyebrow">Phase 2 Â· Section 2.2</div>
  <h1>Advanced<br/><em>LCEL Patterns</em></h1>
  <p class="hero-desc">Branching, routing, fallbacks, retries, and parameter binding â€” the patterns that take chains from toy demos to production-grade systems.</p>
  <div class="hero-pills">
    <span class="hpill hp-r">RunnableBranch</span>
    <span class="hpill hp-o">Dynamic Routing</span>
    <span class="hpill hp-t">.with_fallbacks()</span>
    <span class="hpill hp-b">.with_retry()</span>
    <span class="hpill hp-p">.bind()</span>
  </div>
</div>

<div class="page">

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <!-- 01 â€” RUNNABLEBRANCH                        -->
  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <section id="branch">
    <div class="sh">
      <div class="sh-row"><span class="snum sn-r">01</span><h2 class="stitle">RunnableBranch</h2></div>
      <p class="sdesc">Conditional logic â€” route inputs to different chains based on a predicate</p>
    </div>

    <p style="color:var(--muted);margin-bottom:18px;">
      <code>RunnableBranch</code> is LangChain's <strong>if-elif-else</strong> for chains. You define a list of <code>(condition, chain)</code> pairs. The first condition that returns <code>True</code> wins â€” its chain handles the input. The final argument is always the default fallback chain.
    </p>

    <div class="branch-diagram">
      <div style="font-family:'IBM Plex Mono',monospace;font-size:10px;color:var(--muted2);letter-spacing:2px;margin-bottom:16px;">ROUTING FLOW</div>
      <div class="bd-row">
        <div class="bd-input">{"query": "...", "lang": "fr"}</div>
        <div class="bd-arrow">â†’</div>
        <div class="bd-router">RunnableBranch</div>
        <div class="bd-arrow">â†’</div>
        <div class="bd-branches">
          <div class="bd-branch">
            <div class="bd-cond">lang == "fr"</div>
            <div class="bd-chain bdc-r">french_chain â†’ rÃ©ponse en franÃ§ais</div>
          </div>
          <div class="bd-branch">
            <div class="bd-cond">lang == "es"</div>
            <div class="bd-chain bdc-t">spanish_chain â†’ respuesta en espaÃ±ol</div>
          </div>
          <div class="bd-branch">
            <div class="bd-cond">lang == "de"</div>
            <div class="bd-chain bdc-b">german_chain â†’ Antwort auf Deutsch</div>
          </div>
          <div class="bd-branch">
            <div class="bd-cond">default</div>
            <div class="bd-chain bdc-g">english_chain â†’ answer in English</div>
          </div>
        </div>
      </div>
    </div>

<pre class="labeled"><span class="code-label">RunnableBranch â€” language routing</span>
<span class="kw">from</span> langchain_core.runnables <span class="kw">import</span> <span class="cls">RunnableBranch</span>, <span class="cls">RunnableLambda</span>
<span class="kw">from</span> langchain_core.prompts <span class="kw">import</span> <span class="cls">ChatPromptTemplate</span>
<span class="kw">from</span> langchain_openai <span class="kw">import</span> <span class="cls">ChatOpenAI</span>
<span class="kw">from</span> langchain_core.output_parsers <span class="kw">import</span> <span class="cls">StrOutputParser</span>

model  = <span class="cls">ChatOpenAI</span>(model=<span class="str">"gpt-4o-mini"</span>)
parser = <span class="cls">StrOutputParser</span>()

<span class="cm"># Build a chain per language</span>
<span class="kw">def</span> <span class="fn">make_chain</span>(lang_instruction):
    <span class="kw">return</span> <span class="cls">ChatPromptTemplate</span>.from_template(
        lang_instruction + <span class="str">"\n\nQuestion: {query}"</span>
    ) <span class="op">|</span> model <span class="op">|</span> parser

french_chain  = <span class="fn">make_chain</span>(<span class="str">"RÃ©ponds uniquement en franÃ§ais."</span>)
spanish_chain = <span class="fn">make_chain</span>(<span class="str">"Responde Ãºnicamente en espaÃ±ol."</span>)
default_chain = <span class="fn">make_chain</span>(<span class="str">"Answer in English."</span>)

<span class="cm"># RunnableBranch: list of (condition, chain) + default</span>
branch = <span class="cls">RunnableBranch</span>(
    (<span class="kw">lambda</span> x: x[<span class="str">"lang"</span>] == <span class="str">"fr"</span>,  french_chain),
    (<span class="kw">lambda</span> x: x[<span class="str">"lang"</span>] == <span class="str">"es"</span>,  spanish_chain),
    default_chain,   <span class="cm"># â† no condition = default (must be last)</span>
)

<span class="cm"># Test routing</span>
<span class="fn">print</span>(branch.invoke({<span class="str">"query"</span>: <span class="str">"What is AI?"</span>, <span class="str">"lang"</span>: <span class="str">"fr"</span>}))  <span class="cm"># French</span>
<span class="fn">print</span>(branch.invoke({<span class="str">"query"</span>: <span class="str">"What is AI?"</span>, <span class="str">"lang"</span>: <span class="str">"de"</span>}))  <span class="cm"># English (default)</span>
</pre>

    <div class="box box-g">
      <strong>Real-world use cases</strong>
      Route by query complexity (simple â†’ cheap model, complex â†’ powerful model) Â· Route by user tier (free â†’ limited chain, premium â†’ full chain) Â· Route by detected intent (question â†’ QA chain, complaint â†’ support chain, greeting â†’ chitchat chain)
    </div>
  </section>

  <hr/>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <!-- 02 â€” DYNAMIC ROUTING                       -->
  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <section id="routing">
    <div class="sh">
      <div class="sh-row"><span class="snum sn-o">02</span><h2 class="stitle">Dynamic Routing</h2></div>
      <p class="sdesc">Let the LLM itself decide which chain to use</p>
    </div>

    <p style="color:var(--muted);margin-bottom:18px;">
      <code>RunnableBranch</code> uses deterministic Python conditions. <strong>Dynamic routing</strong> goes further â€” you ask an LLM to classify the input first, then route based on its classification. This is more flexible and handles nuanced cases that hard-coded rules can't.
    </p>

    <div class="box box-o">
      <strong>The pattern</strong>
      Input â†’ <strong>classifier LLM</strong> (returns a label like "technical" / "billing" / "general") â†’ <strong>RunnableLambda router</strong> (reads the label, returns the correct chain) â†’ execute that chain
    </div>

<pre class="labeled"><span class="code-label">Dynamic routing â€” LLM-based intent classification</span>
<span class="kw">from</span> langchain_core.runnables <span class="kw">import</span> <span class="cls">RunnableLambda</span>
<span class="kw">from</span> langchain_core.prompts <span class="kw">import</span> <span class="cls">ChatPromptTemplate</span>
<span class="kw">from</span> langchain_openai <span class="kw">import</span> <span class="cls">ChatOpenAI</span>
<span class="kw">from</span> langchain_core.output_parsers <span class="kw">import</span> <span class="cls">StrOutputParser</span>

model  = <span class="cls">ChatOpenAI</span>(model=<span class="str">"gpt-4o-mini"</span>, temperature=<span class="num">0</span>)
parser = <span class="cls">StrOutputParser</span>()

<span class="cm"># â”€â”€ Step 1: Classifier chain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
<span class="cm"># Returns exactly one of: "technical" | "billing" | "general"</span>
classifier_chain = <span class="cls">ChatPromptTemplate</span>.from_template(
    <span class="str">"""Classify the user question into exactly one category.
Categories: technical, billing, general
Respond with only the category name, nothing else.
Question: {question}"""</span>
) <span class="op">|</span> model <span class="op">|</span> parser

<span class="cm"># â”€â”€ Step 2: Specialist chains â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
technical_chain = <span class="cls">ChatPromptTemplate</span>.from_template(
    <span class="str">"You are a senior engineer. Answer this technical question: {question}"</span>
) <span class="op">|</span> model <span class="op">|</span> parser

billing_chain = <span class="cls">ChatPromptTemplate</span>.from_template(
    <span class="str">"You are a billing specialist. Answer this billing question: {question}"</span>
) <span class="op">|</span> model <span class="op">|</span> parser

general_chain = <span class="cls">ChatPromptTemplate</span>.from_template(
    <span class="str">"You are a helpful assistant. Answer: {question}"</span>
) <span class="op">|</span> model <span class="op">|</span> parser

<span class="cm"># â”€â”€ Step 3: Router function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
<span class="kw">def</span> <span class="fn">route</span>(info: <span class="fn">dict</span>):
    <span class="cm"># info = {"question": "...", "category": "technical"}</span>
    category = info[<span class="str">"category"</span>].strip().lower()
    <span class="kw">if</span> category == <span class="str">"technical"</span>:
        <span class="kw">return</span> technical_chain
    <span class="kw">elif</span> category == <span class="str">"billing"</span>:
        <span class="kw">return</span> billing_chain
    <span class="kw">else</span>:
        <span class="kw">return</span> general_chain

<span class="cm"># â”€â”€ Step 4: Full pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
<span class="kw">from</span> langchain_core.runnables <span class="kw">import</span> <span class="cls">RunnableParallel</span>, <span class="cls">RunnablePassthrough</span>

full_chain = (
    <span class="cls">RunnableParallel</span>({
        <span class="str">"question"</span>: <span class="cls">RunnablePassthrough</span>(),        <span class="cm"># keep original question</span>
        <span class="str">"category"</span>: classifier_chain,               <span class="cm"># classify it</span>
    })
    <span class="op">|</span> <span class="cls">RunnableLambda</span>(<span class="kw">lambda</span> x: <span class="fn">route</span>(x).invoke(x)) <span class="cm"># route + execute</span>
)

<span class="fn">print</span>(full_chain.invoke(<span class="str">"Why is my API returning 429 errors?"</span>))
<span class="cm"># â†’ classified as "technical" â†’ technical_chain handles it</span>

<span class="fn">print</span>(full_chain.invoke(<span class="str">"I was charged twice this month."</span>))
<span class="cm"># â†’ classified as "billing"   â†’ billing_chain handles it</span>
</pre>

    <div class="box box-b">
      <strong>RunnableBranch vs Dynamic Routing â€” when to use which</strong>
      Use <code>RunnableBranch</code> when your conditions are deterministic and fast (checking a field value, string match). Use dynamic routing when the classification itself requires intelligence â€” ambiguous queries, natural language intent detection, or conditions that can't be expressed as simple predicates.
    </div>
  </section>

  <hr/>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <!-- 03 â€” FALLBACKS                             -->
  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <section id="fallbacks">
    <div class="sh">
      <div class="sh-row"><span class="snum sn-t">03</span><h2 class="stitle">Fallbacks with <code>.with_fallbacks()</code></h2></div>
      <p class="sdesc">Gracefully handle failures by automatically trying backup chains</p>
    </div>

    <p style="color:var(--muted);margin-bottom:18px;">
      Any <code>Runnable</code> can have fallbacks. If the primary chain raises an exception â€” rate limit, timeout, content filter, API error â€” LangChain automatically tries each fallback in order until one succeeds. This is essential for production reliability.
    </p>

    <div class="fallback-diagram">
      <div style="font-family:'IBM Plex Mono',monospace;font-size:10px;color:var(--muted2);letter-spacing:2px;margin-bottom:14px;">FALLBACK EXECUTION FLOW</div>
      <div class="fd-row">
        <div class="fd-num fn-fail">1</div>
        <div class="fd-box fdb-primary">gpt-4o (primary)</div>
        <span class="fd-status fs-fail">RateLimitError âœ—</span>
      </div>
      <div class="fd-connector">â†“ automatically tries next...</div>
      <div class="fd-row">
        <div class="fd-num fn-fail">2</div>
        <div class="fd-box fdb-fallback">gpt-4o-mini (fallback 1)</div>
        <span class="fd-status fs-fail">Timeout âœ—</span>
      </div>
      <div class="fd-connector">â†“ automatically tries next...</div>
      <div class="fd-row">
        <div class="fd-num fn-ok">3</div>
        <div class="fd-box fdb-final">claude-3-haiku (fallback 2)</div>
        <span class="fd-status fs-ok">Success âœ“</span>
      </div>
    </div>

<pre class="labeled"><span class="code-label">.with_fallbacks() â€” model redundancy</span>
<span class="kw">from</span> langchain_openai <span class="kw">import</span> <span class="cls">ChatOpenAI</span>
<span class="kw">from</span> langchain_anthropic <span class="kw">import</span> <span class="cls">ChatAnthropic</span>

<span class="cm"># Primary model â€” expensive/powerful</span>
primary = <span class="cls">ChatOpenAI</span>(model=<span class="str">"gpt-4o"</span>)

<span class="cm"># Fallback chain â€” cheaper/more available</span>
fallback_1 = <span class="cls">ChatOpenAI</span>(model=<span class="str">"gpt-4o-mini"</span>)
fallback_2 = <span class="cls">ChatAnthropic</span>(model=<span class="str">"claude-3-haiku-20240307"</span>)

<span class="cm"># Attach fallbacks â€” tried in order on any exception</span>
resilient_model = primary.with_fallbacks([fallback_1, fallback_2])

<span class="cm"># Works exactly like a normal model</span>
response = resilient_model.invoke(<span class="str">"Explain quantum entanglement."</span>)


<span class="cm"># â”€â”€ Fallback on specific exceptions only â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
<span class="kw">from</span> openai <span class="kw">import</span> RateLimitError, APITimeoutError

resilient_model = primary.with_fallbacks(
    [fallback_1],
    exceptions_to_handle=(RateLimitError, APITimeoutError)
    <span class="cm"># Other exceptions (e.g. AuthenticationError) still raise</span>
)


<span class="cm"># â”€â”€ Fallback on entire chains, not just models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
primary_chain = (
    <span class="cls">ChatPromptTemplate</span>.from_template(<span class="str">"Answer as JSON: {question}"</span>)
    <span class="op">|</span> primary
    <span class="op">|</span> <span class="cls">JsonOutputParser</span>()  <span class="cm"># this can fail if model doesn't return valid JSON</span>
)

safe_chain = primary_chain.with_fallbacks([
    <span class="cls">ChatPromptTemplate</span>.from_template(<span class="str">"Answer as plain text: {question}"</span>)
    <span class="op">|</span> fallback_1
    <span class="op">|</span> <span class="cls">StrOutputParser</span>()  <span class="cm"># simpler parser that won't fail</span>
])
</pre>

    <div class="box box-t">
      <strong>Pro tip â€” fallback on chains not just models</strong>
      You can attach <code>.with_fallbacks()</code> to any Runnable, including entire chains. A common pattern: try a chain with a strict <code>PydanticOutputParser</code> (which fails on malformed JSON), fall back to a chain with <code>StrOutputParser</code> that always succeeds.
    </div>

    <div class="box box-r">
      <strong>Warning â€” fallbacks hide bugs</strong>
      Don't use fallbacks as an excuse to ignore errors. Always log which fallback was triggered and why. Silent fallback to a weaker model can degrade output quality without anyone noticing. Use LangSmith tracing to monitor fallback rates.
    </div>
  </section>

  <hr/>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <!-- 04 â€” RETRY                                 -->
  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <section id="retry">
    <div class="sh">
      <div class="sh-row"><span class="snum sn-b">04</span><h2 class="stitle">Retries with <code>.with_retry()</code></h2></div>
      <p class="sdesc">Automatically retry transient failures with exponential backoff</p>
    </div>

    <p style="color:var(--muted);margin-bottom:18px;">
      Retries handle <em>transient</em> failures â€” the same model, the same chain, tried again after a brief wait. Use retries for rate limit errors and temporary outages. Use fallbacks for when you want to try a <em>different</em> chain entirely.
    </p>

    <div class="retry-timeline">
      <div style="font-family:'IBM Plex Mono',monospace;font-size:10px;color:var(--muted2);letter-spacing:2px;margin-bottom:16px;">RETRY WITH EXPONENTIAL BACKOFF</div>
      <div class="rt-row">
        <div class="rt-attempt">attempt 1</div>
        <div class="rt-bar rb-fail" style="width:60px;"></div>
        <div class="rt-status rs-fail">RateLimit</div>
        <div class="rt-wait">wait 1s â†’</div>
      </div>
      <div class="rt-row">
        <div class="rt-attempt">attempt 2</div>
        <div class="rt-bar rb-fail" style="width:60px;"></div>
        <div class="rt-status rs-fail">RateLimit</div>
        <div class="rt-wait">wait 2s â†’</div>
      </div>
      <div class="rt-row">
        <div class="rt-attempt">attempt 3</div>
        <div class="rt-bar rb-fail" style="width:60px;"></div>
        <div class="rt-status rs-fail">Timeout</div>
        <div class="rt-wait">wait 4s â†’</div>
      </div>
      <div class="rt-row">
        <div class="rt-attempt">attempt 4</div>
        <div class="rt-bar rb-ok" style="width:80px;"></div>
        <div class="rt-status rs-ok">Success âœ“</div>
      </div>
    </div>

<pre class="labeled"><span class="code-label">.with_retry() â€” transient failure handling</span>
<span class="kw">from</span> openai <span class="kw">import</span> RateLimitError, APITimeoutError, InternalServerError

model = <span class="cls">ChatOpenAI</span>(model=<span class="str">"gpt-4o"</span>)

<span class="cm"># Basic retry â€” 3 attempts on any exception</span>
retrying_model = model.with_retry(stop_after_attempt=<span class="num">3</span>)

<span class="cm"># Full control</span>
retrying_model = model.with_retry(
    retry_if_exception_type=(RateLimitError, APITimeoutError, InternalServerError),
    wait_exponential_jitter=<span class="op">True</span>,  <span class="cm"># adds randomness to avoid thundering herd</span>
    stop_after_attempt=<span class="num">4</span>,
)

<span class="cm"># Apply to a whole chain, not just the model</span>
chain = (prompt <span class="op">|</span> model <span class="op">|</span> parser).with_retry(
    retry_if_exception_type=(RateLimitError,),
    stop_after_attempt=<span class="num">3</span>,
)

<span class="cm"># Combining retry + fallback (best of both) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
<span class="cm"># Retry the primary a few times, THEN fall back to backup</span>
resilient_chain = (
    (prompt <span class="op">|</span> primary_model <span class="op">|</span> parser)
    .with_retry(stop_after_attempt=<span class="num">2</span>)        <span class="cm"># try primary twice</span>
    .with_fallbacks([backup_chain])             <span class="cm"># then try backup</span>
)
</pre>

    <div class="table-wrap">
      <table>
        <thead><tr><th>Scenario</th><th>Use</th><th>Why</th></tr></thead>
        <tbody>
          <tr><td>Rate limit / 429</td><td>retry</td><td>Same model will work after waiting</td></tr>
          <tr><td>Timeout / 503</td><td>retry or both</td><td>Transient â€” usually resolves itself</td></tr>
          <tr><td>Model unavailable</td><td>fallback</td><td>Need a different model entirely</td></tr>
          <tr><td>Bad JSON output</td><td>fallback</td><td>Retry won't fix the model's behavior</td></tr>
          <tr><td>Auth error / 401</td><td>neither</td><td>Retrying won't help â€” fix your key</td></tr>
        </tbody>
      </table>
    </div>
  </section>

  <hr/>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <!-- 05 â€” BIND                                  -->
  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <section id="bind">
    <div class="sh">
      <div class="sh-row"><span class="snum sn-p">05</span><h2 class="stitle">Binding Parameters with <code>.bind()</code></h2></div>
      <p class="sdesc">Pre-configure any Runnable's parameters â€” tools, stop sequences, temperature, and more</p>
    </div>

    <p style="color:var(--muted);margin-bottom:18px;">
      <code>.bind()</code> attaches fixed keyword arguments to a Runnable, creating a new Runnable with those parameters baked in. It's like Python's <code>functools.partial</code> â€” pre-fill some arguments so you don't have to pass them on every call. Essential for attaching tools to models.
    </p>

    <div class="params-grid">
      <div class="param-card">
        <span class="param-name">tools=</span>
        <p>Attach tool/function definitions to a model. The model can then call these tools. Most common use of <code>.bind()</code>.</p>
      </div>
      <div class="param-card">
        <span class="param-name">stop=</span>
        <p>Define stop sequences â€” tokens that cause the model to stop generating. Useful for structured outputs.</p>
      </div>
      <div class="param-card">
        <span class="param-name">temperature=</span>
        <p>Override temperature per-chain without recreating the model object. Good for deterministic sub-chains.</p>
      </div>
      <div class="param-card">
        <span class="param-name">response_format=</span>
        <p>Force JSON mode on OpenAI models. Guarantees the model outputs valid JSON every time.</p>
      </div>
    </div>

<pre class="labeled"><span class="code-label">.bind() â€” tools, stop sequences, JSON mode</span>
<span class="kw">from</span> langchain_openai <span class="kw">import</span> <span class="cls">ChatOpenAI</span>
<span class="kw">from</span> langchain_core.prompts <span class="kw">import</span> <span class="cls">ChatPromptTemplate</span>
<span class="kw">from</span> langchain_core.output_parsers <span class="kw">import</span> <span class="cls">StrOutputParser</span>

model = <span class="cls">ChatOpenAI</span>(model=<span class="str">"gpt-4o-mini"</span>)

<span class="cm"># â”€â”€ Bind tools to model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
tools = [
    {
        <span class="str">"type"</span>: <span class="str">"function"</span>,
        <span class="str">"function"</span>: {
            <span class="str">"name"</span>: <span class="str">"get_weather"</span>,
            <span class="str">"description"</span>: <span class="str">"Get current weather for a city"</span>,
            <span class="str">"parameters"</span>: {
                <span class="str">"type"</span>: <span class="str">"object"</span>,
                <span class="str">"properties"</span>: {<span class="str">"city"</span>: {<span class="str">"type"</span>: <span class="str">"string"</span>}},
                <span class="str">"required"</span>: [<span class="str">"city"</span>]
            }
        }
    }
]

model_with_tools = model.bind(tools=tools)
<span class="cm"># Now the model knows about get_weather and can call it</span>
response = model_with_tools.invoke(<span class="str">"What's the weather in Cairo?"</span>)


<span class="cm"># â”€â”€ Bind stop sequences â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
<span class="cm"># Stop generating when the model outputs "```"</span>
code_model = model.bind(stop=[<span class="str">"```"</span>])
prompt = <span class="cls">ChatPromptTemplate</span>.from_template(<span class="str">"Write a Python function for {task}:\n\n```python\n"</span>)
chain = prompt <span class="op">|</span> code_model <span class="op">|</span> <span class="cls">StrOutputParser</span>()
<span class="cm"># Output stops exactly at the closing ```, giving clean code</span>


<span class="cm"># â”€â”€ Bind JSON mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
json_model = model.bind(response_format={<span class="str">"type"</span>: <span class="str">"json_object"</span>})
chain = (
    <span class="cls">ChatPromptTemplate</span>.from_template(
        <span class="str">"Extract: name, age, city from: {text}. Respond with JSON."</span>
    )
    <span class="op">|</span> json_model
    <span class="op">|</span> <span class="cls">JsonOutputParser</span>()
)
result = chain.invoke({<span class="str">"text"</span>: <span class="str">"Sarah is 29, she lives in Tokyo."</span>})
<span class="cm"># {"name": "Sarah", "age": 29, "city": "Tokyo"}</span>


<span class="cm"># â”€â”€ bind_tools() â€” the cleaner method for tools â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>
<span class="kw">from</span> langchain_core.tools <span class="kw">import</span> tool

<span class="op">@</span>tool
<span class="kw">def</span> <span class="fn">multiply</span>(a: <span class="fn">int</span>, b: <span class="fn">int</span>) -> <span class="fn">int</span>:
    <span class="str">"""Multiply two numbers."""</span>
    <span class="kw">return</span> a * b

<span class="cm"># bind_tools() is .bind() but LangChain-aware â€” handles schema automatically</span>
model_with_tools = model.bind_tools([multiply])
response = model_with_tools.invoke(<span class="str">"What is 17 times 34?"</span>)
<span class="fn">print</span>(response.tool_calls)  <span class="cm"># [{"name": "multiply", "args": {"a": 17, "b": 34}}]</span>
</pre>

    <div class="box box-p">
      <strong>bind() vs bind_tools()</strong>
      <code>.bind(tools=[...])</code> accepts raw OpenAI-format tool dicts. <code>.bind_tools([tool])</code> accepts LangChain <code>@tool</code> decorated functions and automatically converts them to the correct schema for whatever provider you're using (OpenAI, Anthropic, Gemini). Prefer <code>.bind_tools()</code> when working with LangChain tools.
    </div>
  </section>

  <hr/>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <!-- SUMMARY                                    -->
  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <section id="summary">
    <div class="sh">
      <div class="sh-row"><span class="snum sn-g">âœ“</span><h2 class="stitle">Quick Reference</h2></div>
    </div>

    <div class="summary-grid">
      <div class="sum-card sc-r">
        <h4>ğŸŒ¿ RunnableBranch</h4>
        <ul>
          <li>List of <code>(condition_fn, chain)</code> pairs + default</li>
          <li>First matching condition wins</li>
          <li>Conditions are Python callables â€” deterministic</li>
          <li>Good for known, enumerable categories</li>
        </ul>
      </div>
      <div class="sum-card sc-o">
        <h4>ğŸ§  Dynamic Routing</h4>
        <ul>
          <li>LLM classifies â†’ Python routes</li>
          <li>Handles ambiguous / natural language conditions</li>
          <li>Classify first in parallel, then route</li>
          <li>Adds latency â€” use a fast model to classify</li>
        </ul>
      </div>
      <div class="sum-card sc-t">
        <h4>ğŸ›¡ï¸ .with_fallbacks()</h4>
        <ul>
          <li>Tries backups when primary raises an exception</li>
          <li>Works on any Runnable â€” model or whole chain</li>
          <li>Specify <code>exceptions_to_handle</code> for precision</li>
          <li>Log which fallback triggers â€” don't hide bugs</li>
        </ul>
      </div>
      <div class="sum-card sc-b">
        <h4>ğŸ”„ .with_retry()</h4>
        <ul>
          <li>Retries the <em>same</em> chain on transient errors</li>
          <li>Use for rate limits, timeouts, 5xx errors</li>
          <li>Combine: <code>.with_retry().with_fallbacks()</code></li>
          <li>Don't retry auth errors or bad-output failures</li>
        </ul>
      </div>
      <div class="sum-card sc-p" style="grid-column:span 2;">
        <h4>ğŸ”© .bind()</h4>
        <ul style="display:grid;grid-template-columns:1fr 1fr;gap:0 20px;">
          <li>Pre-fills any keyword arg on a Runnable</li>
          <li>Most used for: <code>tools</code>, <code>stop</code>, <code>response_format</code></li>
          <li>Returns a new Runnable â€” original is unchanged</li>
          <li>Use <code>.bind_tools()</code> for LangChain <code>@tool</code> functions</li>
        </ul>
      </div>
    </div>

    <div class="box box-g" style="margin-top:18px;">
      <strong>The production-grade chain template</strong>
      <code style="font-size:12px;">(prompt | model.bind_tools(tools)).with_retry(stop_after_attempt=2).with_fallbacks([backup_chain])</code>
      â€” This single line gives you tool use, automatic retries on transient failures, and a fallback to a backup model. That's production-ready resilience in one expression.
    </div>
  </section>

</div>

<footer>LangChain Mastery Guide &nbsp;Â·&nbsp; Phase 2, Section 2.2 &nbsp;Â·&nbsp; February 2026 &nbsp;Â·&nbsp; LangChain v0.3+</footer>
</body>
</html>